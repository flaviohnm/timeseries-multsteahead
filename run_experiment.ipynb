{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a14f6287",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# PASSO 1: IMPORTAÇÕES E FUNÇÕES AUXILIARES\n",
    "# ===================================================================\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from abc import ABC, abstractmethod\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Importações do Darts\n",
    "from darts import TimeSeries\n",
    "from darts.models import NBEATSModel, RNNModel, LinearRegressionModel\n",
    "from darts.dataprocessing.transformers import MissingValuesFiller, Scaler\n",
    "from darts.metrics import mape, mase\n",
    "\n",
    "# Importações de modelos e utilidades\n",
    "from pmdarima import auto_arima\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "# Ignorar avisos para uma saída mais limpa\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=UserWarning)\n",
    "print(\"Todas as bibliotecas foram importadas.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0494752",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# PASSO 2: CLASSE DATALOADER\n",
    "# ===================================================================\n",
    "class DataLoader:\n",
    "    def __init__(self, base_path='datasets/'):\n",
    "        self.base_path = base_path\n",
    "        os.makedirs(self.base_path, exist_ok=True)\n",
    "\n",
    "    def load_classic_ts_dataset(self, dataset_name: str) -> pd.Series:\n",
    "        local_path = os.path.join(self.base_path, f\"{dataset_name}.csv\")\n",
    "        if os.path.exists(local_path):\n",
    "            return pd.read_csv(local_path, index_col=0, parse_dates=True).squeeze()\n",
    "        print(\n",
    "            f\"Carregando o dataset '{dataset_name}' da biblioteca e salvando localmente...\")\n",
    "        try:\n",
    "            if dataset_name == 'AirPassengers':\n",
    "                df = sm.datasets.get_rdataset(\"AirPassengers\").data\n",
    "                series = pd.Series(df['value'].values, index=pd.date_range(\n",
    "                    start='1949-01-01', periods=len(df), freq='MS'), name=\"AirPassengers\")\n",
    "            elif dataset_name == 'co2':\n",
    "                data = sm.datasets.co2.load_pandas().data\n",
    "                series = data['co2'].resample('W').mean().ffill().rename(\"CO2\")\n",
    "            elif dataset_name == 'nottem':\n",
    "                df = sm.datasets.get_rdataset(\"nottem\").data\n",
    "                series = pd.Series(df['value'].values, index=pd.date_range(\n",
    "                    start='1920-01-01', periods=len(df), freq='MS'), name=\"NottinghamTemp\")\n",
    "            elif dataset_name == 'JohnsonJohnson':\n",
    "                df = sm.datasets.get_rdataset(\"JohnsonJohnson\").data\n",
    "                series = pd.Series(df['value'].values, index=pd.date_range(\n",
    "                    start='1960-01-01', periods=len(df), freq='QE'), name=\"JohnsonJohnson\")\n",
    "            elif dataset_name == 'UKgas':\n",
    "                df = sm.datasets.get_rdataset(\"UKgas\").data\n",
    "                series = pd.Series(df['value'].values, index=pd.date_range(\n",
    "                    start='1960-01-01', periods=len(df), freq='QE'), name=\"UKGas\")\n",
    "            elif dataset_name == 'Sunspots':\n",
    "                df = sm.datasets.sunspots.load_pandas().data\n",
    "                series = pd.Series(df['SUNACTIVITY'].values, index=pd.to_datetime(\n",
    "                    df['YEAR'], format='%Y'), name=\"Sunspots\")\n",
    "            elif dataset_name == 'Nile':\n",
    "                df = sm.datasets.nile.load_pandas().data.reset_index()\n",
    "                series = pd.Series(df['volume'].values, index=pd.to_datetime(\n",
    "                    df['year'], format='%Y'), name=\"Nile\")\n",
    "            elif dataset_name == 'ukdriverdeaths':\n",
    "                df = sm.datasets.get_rdataset(\"UKDriverDeaths\").data\n",
    "                series = pd.Series(df['value'].values, index=pd.date_range(\n",
    "                    start='1969-01-01', periods=len(df), freq='MS'), name=\"UKDriverDeaths\")\n",
    "            else:\n",
    "                raise ValueError(f\"Dataset '{dataset_name}' não reconhecido.\")\n",
    "            series.to_csv(local_path)\n",
    "            return series\n",
    "        except Exception as e:\n",
    "            print(f\"Erro ao carregar o dataset '{dataset_name}': {e}\")\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e608c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# PASSO 3: DEFINIÇÃO DAS CLASSES DE MODELO E FUNÇÕES AUXILIARES\n",
    "# ==================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b091e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sliding_window_dataset(data, n_in=1, n_out=1):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data)):\n",
    "        end_ix = i + n_in\n",
    "        out_end_ix = end_ix + n_out\n",
    "        if out_end_ix > len(data):\n",
    "            break\n",
    "        X.append(data[i:end_ix])\n",
    "        y.append(data[end_ix:out_end_ix])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "\n",
    "def get_safe_pandas_series(darts_series: TimeSeries) -> pd.Series:\n",
    "    \"\"\"Função auxiliar para converter TimeSeries para pd.Series de forma robusta.\"\"\"\n",
    "    return pd.Series(darts_series.values().flatten(), index=darts_series.time_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "478e4757",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# PASSO 4: CLASSE BASE\n",
    "# ==================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f2aebfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseModel(ABC):\n",
    "    def __init__(self, name: str): self.name = name\n",
    "    @abstractmethod\n",
    "    def fit(self, train_series: TimeSeries, forecast_horizon: int): pass\n",
    "    @abstractmethod\n",
    "    def predict(self, n: int) -> TimeSeries: pass\n",
    "    def __str__(self): return self.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25596b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# MODELO ORIGINAL DO ARTIGO (RECURSIVO-DIRETO)\n",
    "# ===================================================================\n",
    "class HySMF(BaseModel):\n",
    "    def __init__(self, name=\"HyS-MF\", input_chunk_length=24, n_epochs=50):\n",
    "        super().__init__(name)\n",
    "        self.input_chunk_length, self.n_epochs = input_chunk_length, n_epochs\n",
    "        self.arima_model, self.nbeats_experts, self.residuals_train = None, {}, None\n",
    "        self.scaler = Scaler(MinMaxScaler(feature_range=(-1, 1)))\n",
    "\n",
    "    def fit(self, train_series: TimeSeries, forecast_horizon: int):\n",
    "        self.arima_model = auto_arima(get_safe_pandas_series(\n",
    "            train_series), seasonal=False, stepwise=True, suppress_warnings=True)\n",
    "        in_sample_preds = self.arima_model.predict_in_sample()\n",
    "        self.residuals_train = train_series - \\\n",
    "            TimeSeries.from_series(\n",
    "                pd.Series(in_sample_preds, index=train_series.time_index))\n",
    "        residuals_scaled = self.scaler.fit_transform(self.residuals_train)\n",
    "        for h in range(1, forecast_horizon + 1):\n",
    "            expert = NBEATSModel(input_chunk_length=self.input_chunk_length,\n",
    "                                 output_chunk_length=h, n_epochs=self.n_epochs, random_state=42)\n",
    "            expert.fit(residuals_scaled)\n",
    "            self.nbeats_experts[h] = expert\n",
    "\n",
    "    def predict(self, n: int) -> TimeSeries:\n",
    "        arima_forecast = TimeSeries.from_series(\n",
    "            self.arima_model.predict(n_periods=n))\n",
    "        residual_forecasts_np = np.zeros(n)\n",
    "        residuals_scaled = self.scaler.transform(self.residuals_train)\n",
    "        for h in range(1, n + 1):\n",
    "            pred_h = self.nbeats_experts[h].predict(\n",
    "                n=h, series=residuals_scaled)\n",
    "            residual_forecasts_np[h-1] = pred_h.values().flatten()[-1]\n",
    "        residual_forecasts_ts = TimeSeries.from_times_and_values(\n",
    "            times=arima_forecast.time_index, values=residual_forecasts_np, columns=arima_forecast.columns)\n",
    "        residual_forecasts_descaled = self.scaler.inverse_transform(\n",
    "            residual_forecasts_ts)\n",
    "        return arima_forecast + residual_forecasts_descaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5324dc2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# VARIAÇÃO 1 (RECURSIVO-MIMO)\n",
    "# ===================================================================\n",
    "class HyS_MF_MIMO(BaseModel):\n",
    "    def __init__(self, name=\"HyS-MF_MIMO\", n_lags=24, n_epochs=50):\n",
    "        super().__init__(name)\n",
    "        self.n_lags, self.n_epochs = n_lags, n_epochs\n",
    "        self.arima_model, self.nbeats_model, self.residuals_train = None, None, None\n",
    "        self.scaler = Scaler(MinMaxScaler(feature_range=(0, 1)))\n",
    "\n",
    "    def fit(self, train_series: TimeSeries, forecast_horizon: int):\n",
    "        self.arima_model = auto_arima(get_safe_pandas_series(\n",
    "            train_series), seasonal=False, stepwise=True, suppress_warnings=True)\n",
    "        residuals = self.arima_model.resid()\n",
    "        self.residuals_train = TimeSeries.from_series(residuals)\n",
    "        residuals_scaled = self.scaler.fit_transform(self.residuals_train)\n",
    "        self.nbeats_model = NBEATSModel(\n",
    "            input_chunk_length=self.n_lags, output_chunk_length=forecast_horizon, n_epochs=self.n_epochs, random_state=42)\n",
    "        self.nbeats_model.fit(residuals_scaled)\n",
    "\n",
    "    def predict(self, n: int) -> TimeSeries:\n",
    "        arima_forecast = TimeSeries.from_series(\n",
    "            self.arima_model.predict(n_periods=n))\n",
    "        residual_forecast_scaled = self.nbeats_model.predict(\n",
    "            n=n, series=self.scaler.transform(self.residuals_train))\n",
    "        residual_forecast = self.scaler.inverse_transform(\n",
    "            residual_forecast_scaled)\n",
    "        return arima_forecast + residual_forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca350e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# VARIAÇÃO 2 (DIRETO-DIRETO) - SUA CONTRIBUIÇÃO ORIGINAL\n",
    "# ===================================================================\n",
    "# class HyS_MF_Direto(BaseModel):\n",
    "#     \"\"\"\n",
    "#     Versão do HyS-MF com estratégia Direto-Direto.\n",
    "#     \"\"\"\n",
    "\n",
    "#     def __init__(self, name=\"HyS-MF-Direto\", lags=24, n_epochs=50):\n",
    "#         super().__init__(name)\n",
    "#         self.lags, self.n_epochs = lags, n_epochs\n",
    "#         self.linear_models, self.nbeats_experts, self.scalers = {}, {}, {}\n",
    "#         self.residuals_train_dict = {}\n",
    "\n",
    "#     def fit(self, train_series: TimeSeries, forecast_horizon: int):\n",
    "#         for h in range(1, forecast_horizon + 1):\n",
    "#             linear_model = LinearRegressionModel(\n",
    "#                 lags=self.lags, output_chunk_length=h)\n",
    "#             linear_model.fit(train_series)\n",
    "#             self.linear_models[h] = linear_model\n",
    "\n",
    "#             in_sample_preds = linear_model.predict(n=len(train_series))\n",
    "#             residuals_h = train_series - in_sample_preds\n",
    "#             self.residuals_train_dict[h] = residuals_h\n",
    "\n",
    "#             scaler = Scaler(MinMaxScaler(feature_range=(-1, 1)))\n",
    "#             residuals_scaled = scaler.fit_transform(residuals_h)\n",
    "#             self.scalers[h] = scaler\n",
    "\n",
    "#             nbeats_expert = NBEATSModel(\n",
    "#                 input_chunk_length=self.lags, output_chunk_length=h, n_epochs=self.n_epochs, random_state=42)\n",
    "#             nbeats_expert.fit(residuals_scaled)\n",
    "#             self.nbeats_experts[h] = nbeats_expert\n",
    "\n",
    "#     def predict(self, n: int) -> TimeSeries:\n",
    "#         final_forecast_values = np.zeros(n)\n",
    "\n",
    "#         for h in range(1, n + 1):\n",
    "#             linear_pred = self.linear_models[h].predict(\n",
    "#                 n=h).values().flatten()[-1]\n",
    "\n",
    "#             # O modelo N-BEATS usa a série de resíduos na qual foi treinado para prever\n",
    "#             residual_train_h = self.residuals_train_dict[h]\n",
    "#             residuals_pred_scaled = self.nbeats_experts[h].predict(\n",
    "#                 n=h, series=residual_train_h)\n",
    "\n",
    "#             residuals_pred_descaled = self.scalers[h].inverse_transform(\n",
    "#                 residuals_pred_scaled).values().flatten()[-1]\n",
    "\n",
    "#             final_forecast_values[h-1] = linear_pred + residuals_pred_descaled\n",
    "\n",
    "#         start_date = self.residuals_train_dict[1].end_time(\n",
    "#         ) + self.residuals_train_dict[1].freq\n",
    "#         final_index = pd.date_range(\n",
    "#             start=start_date, periods=n, freq=self.residuals_train_dict[1].freq)\n",
    "\n",
    "#         return TimeSeries.from_times_and_values(times=final_index, values=final_forecast_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de6b97c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# MODELOS DE COMPARAÇÃO E BASELINES\n",
    "# ===================================================================\n",
    "class PureARIMA(BaseModel):\n",
    "    def __init__(self, name=\"PureARIMA\"): super().__init__(\n",
    "        name); self.model = None\n",
    "\n",
    "    def fit(self, train_series: TimeSeries, forecast_horizon: int):\n",
    "        self.model = auto_arima(get_safe_pandas_series(\n",
    "            train_series), seasonal=True, m=12, stepwise=True, suppress_warnings=True)\n",
    "\n",
    "    def predict(\n",
    "        self, n: int) -> TimeSeries: return TimeSeries.from_series(self.model.predict(n_periods=n))\n",
    "\n",
    "\n",
    "class ARIMA_MLP(BaseModel):\n",
    "    def __init__(self, name=\"ARIMA-MLP\", n_lags=12, max_iter=500):\n",
    "        super().__init__(name)\n",
    "        self.n_lags, self.max_iter = n_lags, max_iter\n",
    "        self.arima_model, self.mlp_model, self.last_residuals = None, None, None\n",
    "\n",
    "    def fit(self, train_series: TimeSeries, forecast_horizon: int):\n",
    "        self.arima_model = auto_arima(get_safe_pandas_series(\n",
    "            train_series), seasonal=False, stepwise=True, suppress_warnings=True)\n",
    "        residuals = self.arima_model.resid()\n",
    "        self.last_residuals = residuals[-self.n_lags:]\n",
    "        X, y = create_sliding_window_dataset(\n",
    "            residuals, self.n_lags, forecast_horizon)\n",
    "        self.mlp_model = MLPRegressor(hidden_layer_sizes=(\n",
    "            20,), max_iter=self.max_iter, random_state=42)\n",
    "        self.mlp_model.fit(X, y)\n",
    "\n",
    "    def predict(self, n: int) -> TimeSeries:\n",
    "        arima_forecast = TimeSeries.from_series(\n",
    "            self.arima_model.predict(n_periods=n))\n",
    "        input_residuals = self.last_residuals.values.reshape(1, -1)\n",
    "        residual_forecast_values = self.mlp_model.predict(\n",
    "            input_residuals).flatten()\n",
    "        residual_forecast = TimeSeries.from_times_and_values(\n",
    "            times=arima_forecast.time_index, values=residual_forecast_values, columns=arima_forecast.columns)\n",
    "        return arima_forecast + residual_forecast\n",
    "\n",
    "\n",
    "class ARIMA_LSTM(BaseModel):\n",
    "    def __init__(self, name=\"ARIMA-LSTM\", n_lags=24, n_epochs=100):\n",
    "        super().__init__(name)\n",
    "        self.n_lags, self.n_epochs = n_lags, n_epochs\n",
    "        self.arima_model, self.lstm_model, self.residuals_train = None, None, None\n",
    "        self.scaler = Scaler(MinMaxScaler(feature_range=(0, 1)))\n",
    "\n",
    "    def fit(self, train_series: TimeSeries, forecast_horizon: int):\n",
    "        self.arima_model = auto_arima(get_safe_pandas_series(\n",
    "            train_series), seasonal=False, stepwise=True, suppress_warnings=True)\n",
    "        residuals = self.arima_model.resid()\n",
    "        self.residuals_train = TimeSeries.from_series(residuals)\n",
    "        residuals_scaled = self.scaler.fit_transform(self.residuals_train)\n",
    "        self.lstm_model = RNNModel(model='LSTM', input_chunk_length=self.n_lags,\n",
    "                                   output_chunk_length=1, n_epochs=self.n_epochs, random_state=42)\n",
    "        self.lstm_model.fit(residuals_scaled)\n",
    "\n",
    "    def predict(self, n: int) -> TimeSeries:\n",
    "        arima_forecast = TimeSeries.from_series(\n",
    "            self.arima_model.predict(n_periods=n))\n",
    "        residual_forecast_scaled = self.lstm_model.predict(\n",
    "            n=n, series=self.scaler.transform(self.residuals_train))\n",
    "        residual_forecast = self.scaler.inverse_transform(\n",
    "            residual_forecast_scaled)\n",
    "        return arima_forecast + residual_forecast\n",
    "\n",
    "\n",
    "class PureLSTM(BaseModel):\n",
    "\n",
    "    def __init__(self, name=\"PureLSTM\", n_lags=24, n_epochs=100):\n",
    "        super().__init__(name)\n",
    "        self.n_lags, self.n_epochs = n_lags, n_epochs\n",
    "        self.scaler = Scaler(MinMaxScaler(feature_range=(0, 1)))\n",
    "        self.model = None\n",
    "\n",
    "    def fit(self, train_series: TimeSeries, forecast_horizon: int):\n",
    "        train_scaled = self.scaler.fit_transform(train_series)\n",
    "\n",
    "        self.model = RNNModel(\n",
    "            model='LSTM',\n",
    "            input_chunk_length=self.n_lags,\n",
    "            output_chunk_length=1,  # Corrigido para 1\n",
    "            n_epochs=self.n_epochs,\n",
    "            random_state=42\n",
    "        )\n",
    "        self.model.fit(train_scaled)\n",
    "\n",
    "    def predict(self, n: int) -> TimeSeries:\n",
    "        # 1. Prever na escala normalizada\n",
    "        prediction_scaled = self.model.predict(n=n)\n",
    "        \n",
    "        # 2. Reverter para a escala original antes de retornar\n",
    "        return self.scaler.inverse_transform(prediction_scaled)\n",
    "\n",
    "\n",
    "class PureNBEATS(BaseModel):\n",
    "    def __init__(self, name=\"PureNBEATS\", n_lags=24, n_epochs=50):\n",
    "        super().__init__(name)\n",
    "        self.n_lags, self.n_epochs = n_lags, n_epochs\n",
    "        self.scaler = Scaler(MinMaxScaler(feature_range=(0, 1)))\n",
    "        self.model = None\n",
    "\n",
    "    def fit(self, train_series: TimeSeries, forecast_horizon: int):\n",
    "        train_scaled = self.scaler.fit_transform(train_series)\n",
    "        self.model = NBEATSModel(input_chunk_length=self.n_lags,\n",
    "                                 output_chunk_length=forecast_horizon, n_epochs=self.n_epochs, random_state=42)\n",
    "        self.model.fit(train_scaled)\n",
    "\n",
    "    def predict(self, n: int) -> TimeSeries:\n",
    "        # 1. Prever na escala normalizada\n",
    "        prediction_scaled = self.model.predict(n=n)\n",
    "        \n",
    "        # 2. Reverter para a escala original antes de retornar\n",
    "        return self.scaler.inverse_transform(prediction_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f2f9d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# PASSO 4: CLASSE EXPERIMENT RUNNER\n",
    "# ===================================================================\n",
    "class ExperimentRunner:\n",
    "    def __init__(self, datasets: dict, models: list, forecast_horizon: int):\n",
    "        self.datasets, self.models, self.forecast_horizon = datasets, models, forecast_horizon\n",
    "\n",
    "    def run(self):\n",
    "        os.makedirs('results/predictions', exist_ok=True)\n",
    "        os.makedirs('results/metrics', exist_ok=True)\n",
    "        for ds_name, series in tqdm(self.datasets.items(), desc=\"Processando Datasets\"):\n",
    "            train, test = series[:-\n",
    "                                 self.forecast_horizon], series[-self.forecast_horizon:]\n",
    "            for model in tqdm(self.models, desc=f\"Modelos para {ds_name}\", leave=False):\n",
    "                try:\n",
    "                    model.fit(train, self.forecast_horizon)\n",
    "                    prediction = model.predict(self.forecast_horizon)\n",
    "                    pred_df = prediction.to_series().to_frame(name='prediction')\n",
    "                    pred_df.to_csv(\n",
    "                        f'results/predictions/{ds_name}_{model.name}.csv')\n",
    "                    mape_score = mape(test, prediction)\n",
    "                    mase_score = mase(test, prediction, train)\n",
    "                    metrics = {'MAPE': mape_score, 'MASE': mase_score}\n",
    "                    with open(f'results/metrics/{ds_name}_{model.name}.pkl', 'wb') as f:\n",
    "                        pickle.dump(metrics, f)\n",
    "                    print(\n",
    "                        f\"Resultados para {model.name} em {ds_name}: MAPE={mape_score:.2f}%, MASE={mase_score:.3f}\")\n",
    "                except Exception as e:\n",
    "                    print(\n",
    "                        f\"ERRO ao processar o modelo {model.name} no dataset {ds_name}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f99b1a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# PASSO 5: EXECUÇÃO DO EXPERIMENTO\n",
    "# ===================================================================\n",
    "LISTA_DE_DATASETS = ['AirPassengers', 'co2', 'UKgas', 'Sunspots','ukdriverdeaths']\n",
    "data_loader = DataLoader()\n",
    "datasets_darts = {}\n",
    "filler = MissingValuesFiller()\n",
    "for name in LISTA_DE_DATASETS:\n",
    "    pd_series = data_loader.load_classic_ts_dataset(name)\n",
    "    if pd_series is not None:\n",
    "        series_darts = TimeSeries.from_series(\n",
    "            pd_series, fill_missing_dates=True, freq=None)\n",
    "        series_darts = filler.transform(series_darts)\n",
    "        datasets_darts[name] = series_darts\n",
    "\n",
    "models_to_run = [\n",
    "    # --- MODELO PRINCIPAL E VARIAÇÕES ---\n",
    "    HySMF(),               # Modelo Original do Artigo (ARIMA: Recursivo / N-BEATS: Direto)\n",
    "    HyS_MF_MIMO(name=\"HyS-MF_MIMO\"), # Variação 1 (ARIMA: Recursivo / N-BEATS: MIMO)\n",
    "\n",
    "    # --- HÍBRIDOS DE COMPARAÇÃO DA LITERATURA ---\n",
    "    ARIMA_MLP(),           # Híbrido (ARIMA: Recursivo / MLP: MIMO)\n",
    "    ARIMA_LSTM(),          # Híbrido (ARIMA: Recursivo / LSTM: Recursivo, devido à biblioteca)\n",
    "\n",
    "    # --- BASELINES PUROS ---\n",
    "    PureARIMA(),           # Baseline Linear (Estratégia Recursiva)\n",
    "    PureLSTM(),            # Baseline Não-Linear (Estratégia Recursiva, devido à biblioteca)\n",
    "    PureNBEATS()           # Baseline Não-Linear (Estratégia MIMO)\n",
    "]\n",
    "\n",
    "FORECAST_HORIZON = 10\n",
    "runner = ExperimentRunner(datasets_darts, models_to_run, FORECAST_HORIZON)\n",
    "\n",
    "print(\"\\n\\n--- INICIANDO A EXECUÇÃO DOS EXPERIMENTOS ---\")\n",
    "runner.run()\n",
    "print(\"\\n\\n--- TODOS OS EXPERIMENTOS FORAM CONCLUÍDOS ---\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vhybrid_clean",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
