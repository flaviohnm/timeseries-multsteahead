{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4779d175",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Todas as bibliotecas foram importadas.\n"
     ]
    }
   ],
   "source": [
    "# ===================================================================\n",
    "# PASSO 1: IMPORTAÇÕES E CONFIGURAÇÃO\n",
    "# ===================================================================\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "from abc import ABC, abstractmethod\n",
    "import warnings\n",
    "from tqdm.notebook import tqdm\n",
    "import shutil\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Ignorar avisos para uma saída mais limpa\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=UserWarning)\n",
    "\n",
    "# Importações do Darts\n",
    "from darts import TimeSeries\n",
    "from darts.models import NBEATSModel, RNNModel\n",
    "from darts.dataprocessing.transformers import MissingValuesFiller, Scaler\n",
    "from darts.metrics import mape, mase\n",
    "\n",
    "# Importações de modelos e utilidades\n",
    "from pmdarima import auto_arima\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "print(\"Todas as bibliotecas foram importadas.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "85c6393d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# PASSO 2: CLASSE DATALOADER\n",
    "# ===================================================================\n",
    "class DataLoader:\n",
    "    def __init__(self, base_path='datasets/'):\n",
    "        self.base_path = base_path\n",
    "        os.makedirs(self.base_path, exist_ok=True)\n",
    "    def load_classic_ts_dataset(self, dataset_name: str) -> pd.Series:\n",
    "        local_path = os.path.join(self.base_path, f\"{dataset_name}.csv\")\n",
    "        if os.path.exists(local_path):\n",
    "            return pd.read_csv(local_path, index_col=0, parse_dates=True).squeeze()\n",
    "        print(f\"Carregando o dataset '{dataset_name}' da biblioteca e salvando localmente...\")\n",
    "        try:\n",
    "            if dataset_name == 'AirPassengers':\n",
    "                df = sm.datasets.get_rdataset(\"AirPassengers\").data\n",
    "                series = pd.Series(df['value'].values, index=pd.date_range(start='1949-01-01', periods=len(df), freq='MS'), name=\"AirPassengers\")\n",
    "            elif dataset_name == 'co2':\n",
    "                data = sm.datasets.co2.load_pandas().data\n",
    "                series = data['co2'].resample('W').mean().ffill().rename(\"CO2\")\n",
    "            elif dataset_name == 'nottem':\n",
    "                df = sm.datasets.get_rdataset(\"nottem\").data\n",
    "                series = pd.Series(df['value'].values, index=pd.date_range(start='1920-01-01', periods=len(df), freq='MS'), name=\"NottinghamTemp\")\n",
    "            elif dataset_name == 'JohnsonJohnson':\n",
    "                df = sm.datasets.get_rdataset(\"JohnsonJohnson\").data\n",
    "                series = pd.Series(df['value'].values, index=pd.date_range(start='1960-01-01', periods=len(df), freq='QE'), name=\"JohnsonJohnson\")\n",
    "            elif dataset_name == 'UKgas':\n",
    "                df = sm.datasets.get_rdataset(\"UKgas\").data\n",
    "                series = pd.Series(df['value'].values, index=pd.date_range(start='1960-01-01', periods=len(df), freq='QE'), name=\"UKGas\")\n",
    "            elif dataset_name == 'Sunspots':\n",
    "                df = sm.datasets.sunspots.load_pandas().data\n",
    "                series = pd.Series(df['SUNACTIVITY'].values, index=pd.to_datetime(df['YEAR'], format='%Y'), name=\"Sunspots\")\n",
    "            elif dataset_name == 'Nile':\n",
    "                df = sm.datasets.nile.load_pandas().data.reset_index()\n",
    "                series = pd.Series(df['volume'].values, index=pd.to_datetime(df['year'], format='%Y'), name=\"Nile\")\n",
    "            elif dataset_name == 'ukdriverdeaths':\n",
    "                df = sm.datasets.get_rdataset(\"UKDriverDeaths\").data\n",
    "                series = pd.Series(df['value'].values, index=pd.date_range(start='1969-01-01', periods=len(df), freq='MS'), name=\"UKDriverDeaths\")\n",
    "            else:\n",
    "                raise ValueError(f\"Dataset '{dataset_name}' não reconhecido.\")\n",
    "            series.to_csv(local_path)\n",
    "            return series\n",
    "        except Exception as e:\n",
    "            print(f\"Erro ao carregar o dataset '{dataset_name}': {e}\"); return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c6bccdc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# PASSO 3: DEFINIÇÃO DO FRAMEWORK HÍBRIDO (COM CORREÇÃO FINAL)\n",
    "# ===================================================================\n",
    "class BaseModel(ABC):\n",
    "    def __init__(self, name: str): self.name = name\n",
    "    @abstractmethod\n",
    "    def fit(self, train_series: TimeSeries, forecast_horizon: int): pass\n",
    "    @abstractmethod\n",
    "    def predict(self, n: int) -> TimeSeries: pass\n",
    "    def __str__(self): return self.name\n",
    "\n",
    "def _get_safe_pandas_series(darts_series: TimeSeries) -> pd.Series:\n",
    "    return pd.Series(darts_series.values().flatten(), index=darts_series.time_index)\n",
    "\n",
    "class HybridForecastingFramework(BaseModel):\n",
    "    def __init__(self, non_linear_model_class, strategy='mimo', model_name=None, **kwargs):\n",
    "        self.non_linear_model_class = non_linear_model_class\n",
    "        self.strategy = strategy\n",
    "        self.non_linear_params = kwargs\n",
    "        name = model_name or f\"ARIMA-{non_linear_model_class.__name__}-{strategy.upper()}\"\n",
    "        super().__init__(name)\n",
    "        self.arima_model, self.non_linear_model, self.residuals_train = None, None, None\n",
    "        self.scaler = Scaler(MinMaxScaler(feature_range=(-1, 1)))\n",
    "\n",
    "    def fit(self, train_series: TimeSeries, forecast_horizon: int):\n",
    "        # 1. Treinamento do Componente Linear\n",
    "        self.arima_model = auto_arima(_get_safe_pandas_series(train_series), seasonal=False, stepwise=True, suppress_warnings=True)\n",
    "        \n",
    "        # 2. **CORREÇÃO DEFINITIVA:** Obter resíduos com o método .resid()\n",
    "        residuals_pd = self.arima_model.resid()\n",
    "        self.residuals_train = TimeSeries.from_series(residuals_pd)\n",
    "        \n",
    "        residuals_scaled = self.scaler.fit_transform(self.residuals_train)\n",
    "\n",
    "        # 3. Treinamento do Componente Não-Linear\n",
    "        if self.strategy == 'direct':\n",
    "            self.non_linear_model = {}\n",
    "            for h in range(1, forecast_horizon + 1):\n",
    "                expert_params = self.non_linear_params.copy(); expert_params['output_chunk_length'] = h\n",
    "                expert = self.non_linear_model_class(**expert_params)\n",
    "                expert.fit(residuals_scaled)\n",
    "                self.non_linear_model[h] = expert\n",
    "        elif self.strategy in ['mimo', 'recursive']:\n",
    "            self.non_linear_params['output_chunk_length'] = forecast_horizon if self.strategy == 'mimo' else 1\n",
    "            self.non_linear_model = self.non_linear_model_class(**self.non_linear_params)\n",
    "            self.non_linear_model.fit(residuals_scaled)\n",
    "\n",
    "    def predict(self, n: int) -> TimeSeries:\n",
    "        arima_forecast = TimeSeries.from_series(self.arima_model.predict(n_periods=n))\n",
    "        residuals_scaled = self.scaler.transform(self.residuals_train)\n",
    "        \n",
    "        if self.strategy == 'direct':\n",
    "            forecasts_np = np.zeros(n)\n",
    "            for h in range(1, n + 1):\n",
    "                pred_h = self.non_linear_model[h].predict(n=h, series=residuals_scaled)\n",
    "                forecasts_np[h-1] = pred_h.values().flatten()[-1]\n",
    "            pred_ts_scaled = TimeSeries.from_times_and_values(arima_forecast.time_index, forecasts_np)\n",
    "        else:\n",
    "            pred_ts_scaled = self.non_linear_model.predict(n=n, series=residuals_scaled)\n",
    "            \n",
    "        residual_forecast = self.scaler.inverse_transform(pred_ts_scaled)\n",
    "        return arima_forecast + residual_forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "805c1038",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelos Puros (Baselines)\n",
    "class PureARIMA(BaseModel):\n",
    "    def __init__(self, name=\"PureARIMA\"): super().__init__(name); self.model = None\n",
    "    def fit(self, train_series: TimeSeries, forecast_horizon: int):\n",
    "        self.model = auto_arima(_get_safe_pandas_series(train_series), seasonal=True, m=12, stepwise=True, suppress_warnings=True)\n",
    "    def predict(self, n: int) -> TimeSeries: return TimeSeries.from_series(self.model.predict(n_periods=n))\n",
    "\n",
    "class PureLSTM(BaseModel):\n",
    "    def __init__(self, name=\"PureLSTM\", n_lags=24, n_epochs=100):\n",
    "        super().__init__(name); self.n_lags, self.n_epochs = n_lags, n_epochs\n",
    "        self.scaler = Scaler(MinMaxScaler(feature_range=(0, 1))); self.model = None\n",
    "    def fit(self, train_series: TimeSeries, forecast_horizon: int):\n",
    "        train_scaled = self.scaler.fit_transform(train_series)\n",
    "        self.model = RNNModel(model='LSTM', input_chunk_length=self.n_lags, output_chunk_length=1, n_epochs=self.n_epochs, random_state=42)\n",
    "        self.model.fit(train_scaled)\n",
    "    def predict(self, n: int) -> TimeSeries:\n",
    "        prediction_scaled = self.model.predict(n=n)\n",
    "        return self.scaler.inverse_transform(prediction_scaled)\n",
    "\n",
    "class PureNBEATS(BaseModel):\n",
    "    def __init__(self, name=\"PureNBEATS\", n_lags=24, n_epochs=100):\n",
    "        super().__init__(name); self.n_lags, self.n_epochs = n_lags, n_epochs\n",
    "        self.scaler = Scaler(MinMaxScaler(feature_range=(0, 1))); self.model = None\n",
    "    def fit(self, train_series: TimeSeries, forecast_horizon: int):\n",
    "        train_scaled = self.scaler.fit_transform(train_series)\n",
    "        self.model = NBEATSModel(input_chunk_length=self.n_lags, output_chunk_length=forecast_horizon, n_epochs=self.n_epochs, random_state=42)\n",
    "        self.model.fit(train_scaled)\n",
    "    def predict(self, n: int) -> TimeSeries: return self.model.predict(n=n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f3fdaf77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# PASSO 4: CLASSE EXPERIMENT RUNNER\n",
    "# ===================================================================\n",
    "class ExperimentRunner:\n",
    "    # ... (código completo da classe ExperimentRunner, sem alterações) ...\n",
    "    def __init__(self, datasets: dict, models: list, forecast_horizon: int):\n",
    "        self.datasets, self.models, self.forecast_horizon = datasets, models, forecast_horizon\n",
    "    def run(self):\n",
    "        if os.path.exists('results/'): shutil.rmtree('results/')\n",
    "        os.makedirs('results/predictions', exist_ok=True); os.makedirs('results/metrics', exist_ok=True)\n",
    "        for ds_name, series in tqdm(self.datasets.items(), desc=\"Processando Datasets\"):\n",
    "            train, test = series[:-self.forecast_horizon], series[-self.forecast_horizon:]\n",
    "            for model in tqdm(self.models, desc=f\"Modelos para {ds_name}\", leave=False):\n",
    "                try:\n",
    "                    model.fit(train, self.forecast_horizon)\n",
    "                    prediction = model.predict(self.forecast_horizon)\n",
    "                    pred_df = prediction.to_series().to_frame(name='prediction')\n",
    "                    pred_df.to_csv(f'results/predictions/{ds_name}_{model}.csv')\n",
    "                    mape_score = mape(test, prediction)\n",
    "                    mase_score = mase(test, prediction, train)\n",
    "                    metrics = {'MAPE': mape_score, 'MASE': mase_score}\n",
    "                    with open(f'results/metrics/{ds_name}_{model}.pkl', 'wb') as f: pickle.dump(metrics, f)\n",
    "                    print(f\"Resultados para {model}: MAPE={mape_score:.2f}%, MASE={mase_score:.3f}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"ERRO ao processar o modelo {model} no dataset {ds_name}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c74bed00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "--- INICIANDO A EXECUÇÃO DOS EXPERIMENTOS COM O NOVO FRAMEWORK ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e32928ace71743c692d8b912b3a4a4a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processando Datasets:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e91bbfbba09b4feeb3246b9141ceb0c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Modelos para AirPassengers:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name            | Type             | Params | Mode \n",
      "-------------------------------------------------------------\n",
      "0 | criterion       | MSELoss          | 0      | train\n",
      "1 | train_criterion | MSELoss          | 0      | train\n",
      "2 | val_criterion   | MSELoss          | 0      | train\n",
      "3 | train_metrics   | MetricCollection | 0      | train\n",
      "4 | val_metrics     | MetricCollection | 0      | train\n",
      "5 | stacks          | ModuleList       | 6.2 M  | train\n",
      "-------------------------------------------------------------\n",
      "6.2 M     Trainable params\n",
      "1.4 K     Non-trainable params\n",
      "6.2 M     Total params\n",
      "24.780    Total estimated model params size (MB)\n",
      "396       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9319bef65cd4cacb292d4fbb14dc07c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name            | Type             | Params | Mode \n",
      "-------------------------------------------------------------\n",
      "0 | criterion       | MSELoss          | 0      | train\n",
      "1 | train_criterion | MSELoss          | 0      | train\n",
      "2 | val_criterion   | MSELoss          | 0      | train\n",
      "3 | train_metrics   | MetricCollection | 0      | train\n",
      "4 | val_metrics     | MetricCollection | 0      | train\n",
      "5 | stacks          | ModuleList       | 6.2 M  | train\n",
      "-------------------------------------------------------------\n",
      "6.2 M     Trainable params\n",
      "1.4 K     Non-trainable params\n",
      "6.2 M     Total params\n",
      "24.780    Total estimated model params size (MB)\n",
      "396       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "035502c8720c475a8dc4a0b25bebdc4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name            | Type             | Params | Mode \n",
      "-------------------------------------------------------------\n",
      "0 | criterion       | MSELoss          | 0      | train\n",
      "1 | train_criterion | MSELoss          | 0      | train\n",
      "2 | val_criterion   | MSELoss          | 0      | train\n",
      "3 | train_metrics   | MetricCollection | 0      | train\n",
      "4 | val_metrics     | MetricCollection | 0      | train\n",
      "5 | stacks          | ModuleList       | 6.2 M  | train\n",
      "-------------------------------------------------------------\n",
      "6.2 M     Trainable params\n",
      "1.4 K     Non-trainable params\n",
      "6.2 M     Total params\n",
      "24.781    Total estimated model params size (MB)\n",
      "396       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e087e7404194d398f07fa5b5f3b7a25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name            | Type             | Params | Mode \n",
      "-------------------------------------------------------------\n",
      "0 | criterion       | MSELoss          | 0      | train\n",
      "1 | train_criterion | MSELoss          | 0      | train\n",
      "2 | val_criterion   | MSELoss          | 0      | train\n",
      "3 | train_metrics   | MetricCollection | 0      | train\n",
      "4 | val_metrics     | MetricCollection | 0      | train\n",
      "5 | stacks          | ModuleList       | 6.2 M  | train\n",
      "-------------------------------------------------------------\n",
      "6.2 M     Trainable params\n",
      "1.4 K     Non-trainable params\n",
      "6.2 M     Total params\n",
      "24.782    Total estimated model params size (MB)\n",
      "396       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "484479347d174bafa72ee9314eccfefc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name            | Type             | Params | Mode \n",
      "-------------------------------------------------------------\n",
      "0 | criterion       | MSELoss          | 0      | train\n",
      "1 | train_criterion | MSELoss          | 0      | train\n",
      "2 | val_criterion   | MSELoss          | 0      | train\n",
      "3 | train_metrics   | MetricCollection | 0      | train\n",
      "4 | val_metrics     | MetricCollection | 0      | train\n",
      "5 | stacks          | ModuleList       | 6.2 M  | train\n",
      "-------------------------------------------------------------\n",
      "6.2 M     Trainable params\n",
      "1.4 K     Non-trainable params\n",
      "6.2 M     Total params\n",
      "24.782    Total estimated model params size (MB)\n",
      "396       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac6d7271875f4ed686ca793ba0f20d2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name            | Type             | Params | Mode \n",
      "-------------------------------------------------------------\n",
      "0 | criterion       | MSELoss          | 0      | train\n",
      "1 | train_criterion | MSELoss          | 0      | train\n",
      "2 | val_criterion   | MSELoss          | 0      | train\n",
      "3 | train_metrics   | MetricCollection | 0      | train\n",
      "4 | val_metrics     | MetricCollection | 0      | train\n",
      "5 | stacks          | ModuleList       | 6.2 M  | train\n",
      "-------------------------------------------------------------\n",
      "6.2 M     Trainable params\n",
      "1.4 K     Non-trainable params\n",
      "6.2 M     Total params\n",
      "24.783    Total estimated model params size (MB)\n",
      "396       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3c9f130507b47b3a74812442022f9cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name            | Type             | Params | Mode \n",
      "-------------------------------------------------------------\n",
      "0 | criterion       | MSELoss          | 0      | train\n",
      "1 | train_criterion | MSELoss          | 0      | train\n",
      "2 | val_criterion   | MSELoss          | 0      | train\n",
      "3 | train_metrics   | MetricCollection | 0      | train\n",
      "4 | val_metrics     | MetricCollection | 0      | train\n",
      "5 | stacks          | ModuleList       | 6.2 M  | train\n",
      "-------------------------------------------------------------\n",
      "6.2 M     Trainable params\n",
      "1.4 K     Non-trainable params\n",
      "6.2 M     Total params\n",
      "24.784    Total estimated model params size (MB)\n",
      "396       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8baa88e95f6a4fd994b9b174b30da736",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ===================================================================\n",
    "# PASSO 5: EXECUÇÃO DO EXPERIMENTO\n",
    "# ===================================================================\n",
    "# --- Carregando os Dados ---\n",
    "# LISTA_DE_DATASETS = ['AirPassengers', 'Nile', 'UKgas', 'Sunspots', 'ukdriverdeaths']\n",
    "LISTA_DE_DATASETS = ['AirPassengers']\n",
    "data_loader = DataLoader()\n",
    "datasets_darts = {}\n",
    "filler = MissingValuesFiller()\n",
    "for name in LISTA_DE_DATASETS:\n",
    "    pd_series = data_loader.load_classic_ts_dataset(name)\n",
    "    if pd_series is not None:\n",
    "        datasets_darts[name] = filler.transform(TimeSeries.from_series(pd_series, fill_missing_dates=True, freq=None))\n",
    "\n",
    "# --- Definindo a Lista COMPLETA de Modelos para o Experimento ---\n",
    "models_to_run = [\n",
    "    HybridForecastingFramework(non_linear_model_class=NBEATSModel, strategy='direct', model_name=\"HyS-MF\", input_chunk_length=24, n_epochs=100, random_state=42),\n",
    "    HybridForecastingFramework(non_linear_model_class=NBEATSModel, strategy='mimo', model_name=\"HyS-MF_MIMO\", input_chunk_length=24, n_epochs=100, random_state=42),\n",
    "    HybridForecastingFramework(non_linear_model_class=RNNModel, strategy='recursive', model_name=\"ARIMA-LSTM\", model='LSTM', input_chunk_length=24, n_epochs=100, random_state=42),\n",
    "    PureARIMA(), \n",
    "    PureLSTM(n_lags=24, n_epochs=100),\n",
    "    PureNBEATS(n_lags=24, n_epochs=100)\n",
    "]\n",
    "\n",
    "# --- Executando o motor de experimentos ---\n",
    "FORECAST_HORIZON = 12\n",
    "runner = ExperimentRunner(datasets_darts, models_to_run, FORECAST_HORIZON)\n",
    "\n",
    "print(\"\\n\\n--- INICIANDO A EXECUÇÃO DOS EXPERIMENTOS COM O NOVO FRAMEWORK ---\")\n",
    "runner.run()\n",
    "print(\"\\n\\n--- TODOS OS EXPERIMENTOS FORAM CONCLUÍDOS ---\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vhybrid_clean",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
