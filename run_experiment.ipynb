{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a14f6287",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Todas as bibliotecas foram importadas.\n"
     ]
    }
   ],
   "source": [
    "# ===================================================================\n",
    "# PASSO 1: IMPORTAÇÕES E FUNÇÕES AUXILIARES\n",
    "# ===================================================================\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from abc import ABC, abstractmethod\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Importações do Darts\n",
    "from darts import TimeSeries\n",
    "from darts.models import NBEATSModel, RNNModel, LinearRegressionModel\n",
    "from darts.dataprocessing.transformers import MissingValuesFiller, Scaler\n",
    "from darts.metrics import mape, mase\n",
    "\n",
    "# Importações de modelos e utilidades\n",
    "from pmdarima import auto_arima\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "# Ignorar avisos para uma saída mais limpa\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=UserWarning)\n",
    "print(\"Todas as bibliotecas foram importadas.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c0494752",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# PASSO 2: CLASSE DATALOADER\n",
    "# ===================================================================\n",
    "class DataLoader:\n",
    "    def __init__(self, base_path='datasets/'):\n",
    "        self.base_path = base_path\n",
    "        os.makedirs(self.base_path, exist_ok=True)\n",
    "\n",
    "    def load_classic_ts_dataset(self, dataset_name: str) -> pd.Series:\n",
    "        local_path = os.path.join(self.base_path, f\"{dataset_name}.csv\")\n",
    "        if os.path.exists(local_path):\n",
    "            return pd.read_csv(local_path, index_col=0, parse_dates=True).squeeze()\n",
    "        print(\n",
    "            f\"Carregando o dataset '{dataset_name}' da biblioteca e salvando localmente...\")\n",
    "        try:\n",
    "            if dataset_name == 'AirPassengers':\n",
    "                df = sm.datasets.get_rdataset(\"AirPassengers\").data\n",
    "                series = pd.Series(df['value'].values, index=pd.date_range(\n",
    "                    start='1949-01-01', periods=len(df), freq='MS'), name=\"AirPassengers\")\n",
    "            elif dataset_name == 'co2':\n",
    "                data = sm.datasets.co2.load_pandas().data\n",
    "                series = data['co2'].resample('W').mean().ffill().rename(\"CO2\")\n",
    "            elif dataset_name == 'nottem':\n",
    "                df = sm.datasets.get_rdataset(\"nottem\").data\n",
    "                series = pd.Series(df['value'].values, index=pd.date_range(\n",
    "                    start='1920-01-01', periods=len(df), freq='MS'), name=\"NottinghamTemp\")\n",
    "            elif dataset_name == 'JohnsonJohnson':\n",
    "                df = sm.datasets.get_rdataset(\"JohnsonJohnson\").data\n",
    "                series = pd.Series(df['value'].values, index=pd.date_range(\n",
    "                    start='1960-01-01', periods=len(df), freq='QE'), name=\"JohnsonJohnson\")\n",
    "            elif dataset_name == 'UKgas':\n",
    "                df = sm.datasets.get_rdataset(\"UKgas\").data\n",
    "                series = pd.Series(df['value'].values, index=pd.date_range(\n",
    "                    start='1960-01-01', periods=len(df), freq='QE'), name=\"UKGas\")\n",
    "            elif dataset_name == 'Sunspots':\n",
    "                df = sm.datasets.sunspots.load_pandas().data\n",
    "                series = pd.Series(df['SUNACTIVITY'].values, index=pd.to_datetime(\n",
    "                    df['YEAR'], format='%Y'), name=\"Sunspots\")\n",
    "            elif dataset_name == 'Nile':\n",
    "                df = sm.datasets.nile.load_pandas().data.reset_index()\n",
    "                series = pd.Series(df['volume'].values, index=pd.to_datetime(\n",
    "                    df['year'], format='%Y'), name=\"Nile\")\n",
    "            elif dataset_name == 'ukdriverdeaths':\n",
    "                df = sm.datasets.get_rdataset(\"UKDriverDeaths\").data\n",
    "                series = pd.Series(df['value'].values, index=pd.date_range(\n",
    "                    start='1969-01-01', periods=len(df), freq='MS'), name=\"UKDriverDeaths\")\n",
    "            else:\n",
    "                raise ValueError(f\"Dataset '{dataset_name}' não reconhecido.\")\n",
    "            series.to_csv(local_path)\n",
    "            return series\n",
    "        except Exception as e:\n",
    "            print(f\"Erro ao carregar o dataset '{dataset_name}': {e}\")\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f3e608c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# PASSO 3: DEFINIÇÃO DAS CLASSES DE MODELO E FUNÇÕES AUXILIARES\n",
    "# ==================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5b091e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sliding_window_dataset(data, n_in=1, n_out=1):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data)):\n",
    "        end_ix = i + n_in\n",
    "        out_end_ix = end_ix + n_out\n",
    "        if out_end_ix > len(data):\n",
    "            break\n",
    "        X.append(data[i:end_ix])\n",
    "        y.append(data[end_ix:out_end_ix])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "\n",
    "def get_safe_pandas_series(darts_series: TimeSeries) -> pd.Series:\n",
    "    \"\"\"Função auxiliar para converter TimeSeries para pd.Series de forma robusta.\"\"\"\n",
    "    return pd.Series(darts_series.values().flatten(), index=darts_series.time_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "478e4757",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# PASSO 4: CLASSE BASE\n",
    "# ==================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3f2aebfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseModel(ABC):\n",
    "    def __init__(self, name: str): self.name = name\n",
    "    @abstractmethod\n",
    "    def fit(self, train_series: TimeSeries, forecast_horizon: int): pass\n",
    "    @abstractmethod\n",
    "    def predict(self, n: int) -> TimeSeries: pass\n",
    "    def __str__(self): return self.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "25596b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# MODELO ORIGINAL DO ARTIGO (RECURSIVO-DIRETO)\n",
    "# ===================================================================\n",
    "class HySMF(BaseModel):\n",
    "    def __init__(self, name=\"HyS-MF\", input_chunk_length=24, n_epochs=100):\n",
    "        super().__init__(name)\n",
    "        self.input_chunk_length, self.n_epochs = input_chunk_length, n_epochs\n",
    "        self.arima_model, self.nbeats_experts, self.residuals_train = None, {}, None\n",
    "        self.scaler = Scaler(MinMaxScaler(feature_range=(-1, 1)))\n",
    "\n",
    "    def fit(self, train_series: TimeSeries, forecast_horizon: int):\n",
    "        self.arima_model = auto_arima(get_safe_pandas_series(\n",
    "            train_series), seasonal=False, stepwise=True, suppress_warnings=True)\n",
    "        in_sample_preds = self.arima_model.predict_in_sample()\n",
    "        self.residuals_train = train_series - \\\n",
    "            TimeSeries.from_series(\n",
    "                pd.Series(in_sample_preds, index=train_series.time_index))\n",
    "        residuals_scaled = self.scaler.fit_transform(self.residuals_train)\n",
    "        for h in range(1, forecast_horizon + 1):\n",
    "            expert = NBEATSModel(input_chunk_length=self.input_chunk_length,\n",
    "                                 output_chunk_length=h, n_epochs=self.n_epochs, random_state=42)\n",
    "            expert.fit(residuals_scaled)\n",
    "            self.nbeats_experts[h] = expert\n",
    "\n",
    "    def predict(self, n: int) -> TimeSeries:\n",
    "        arima_forecast = TimeSeries.from_series(\n",
    "            self.arima_model.predict(n_periods=n))\n",
    "        residual_forecasts_np = np.zeros(n)\n",
    "        residuals_scaled = self.scaler.transform(self.residuals_train)\n",
    "        for h in range(1, n + 1):\n",
    "            pred_h = self.nbeats_experts[h].predict(\n",
    "                n=h, series=residuals_scaled)\n",
    "            residual_forecasts_np[h-1] = pred_h.values().flatten()[-1]\n",
    "        residual_forecasts_ts = TimeSeries.from_times_and_values(\n",
    "            times=arima_forecast.time_index, values=residual_forecasts_np, columns=arima_forecast.columns)\n",
    "        residual_forecasts_descaled = self.scaler.inverse_transform(\n",
    "            residual_forecasts_ts)\n",
    "        return arima_forecast + residual_forecasts_descaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5324dc2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# VARIAÇÃO 1 (RECURSIVO-MIMO)\n",
    "# ===================================================================\n",
    "class HyS_MF_MIMO(BaseModel):\n",
    "    def __init__(self, name=\"HyS-MF_MIMO\", n_lags=24, n_epochs=100):\n",
    "        super().__init__(name)\n",
    "        self.n_lags, self.n_epochs = n_lags, n_epochs\n",
    "        self.arima_model, self.nbeats_model, self.residuals_train = None, None, None\n",
    "        self.scaler = Scaler(MinMaxScaler(feature_range=(0, 1)))\n",
    "\n",
    "    def fit(self, train_series: TimeSeries, forecast_horizon: int):\n",
    "        self.arima_model = auto_arima(get_safe_pandas_series(\n",
    "            train_series), seasonal=False, stepwise=True, suppress_warnings=True)\n",
    "        residuals = self.arima_model.resid()\n",
    "        self.residuals_train = TimeSeries.from_series(residuals)\n",
    "        residuals_scaled = self.scaler.fit_transform(self.residuals_train)\n",
    "        self.nbeats_model = NBEATSModel(\n",
    "            input_chunk_length=self.n_lags, output_chunk_length=forecast_horizon, n_epochs=self.n_epochs, random_state=42)\n",
    "        self.nbeats_model.fit(residuals_scaled)\n",
    "\n",
    "    def predict(self, n: int) -> TimeSeries:\n",
    "        arima_forecast = TimeSeries.from_series(\n",
    "            self.arima_model.predict(n_periods=n))\n",
    "        residual_forecast_scaled = self.nbeats_model.predict(\n",
    "            n=n, series=self.scaler.transform(self.residuals_train))\n",
    "        residual_forecast = self.scaler.inverse_transform(\n",
    "            residual_forecast_scaled)\n",
    "        return arima_forecast + residual_forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9de6b97c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# MODELOS DE COMPARAÇÃO E BASELINES\n",
    "# ===================================================================\n",
    "class PureARIMA(BaseModel):\n",
    "    def __init__(self, name=\"PureARIMA\"): super().__init__(\n",
    "        name); self.model = None\n",
    "\n",
    "    def fit(self, train_series: TimeSeries, forecast_horizon: int):\n",
    "        self.model = auto_arima(get_safe_pandas_series(\n",
    "            train_series), seasonal=True, m=12, stepwise=True, suppress_warnings=True)\n",
    "\n",
    "    def predict(\n",
    "        self, n: int) -> TimeSeries: return TimeSeries.from_series(self.model.predict(n_periods=n))\n",
    "\n",
    "\n",
    "class ARIMA_MLP(BaseModel):\n",
    "    def __init__(self, name=\"ARIMA-MLP\", n_lags=24, max_iter=1000):\n",
    "        super().__init__(name)\n",
    "        self.n_lags, self.max_iter = n_lags, max_iter\n",
    "        self.arima_model, self.mlp_model, self.last_residuals = None, None, None\n",
    "\n",
    "    def fit(self, train_series: TimeSeries, forecast_horizon: int):\n",
    "        self.arima_model = auto_arima(get_safe_pandas_series(\n",
    "            train_series), seasonal=False, stepwise=True, suppress_warnings=True)\n",
    "        residuals = self.arima_model.resid()\n",
    "        self.last_residuals = residuals[-self.n_lags:]\n",
    "        X, y = create_sliding_window_dataset(\n",
    "            residuals, self.n_lags, forecast_horizon)\n",
    "        self.mlp_model = MLPRegressor(hidden_layer_sizes=(\n",
    "            20,), max_iter=self.max_iter, random_state=42)\n",
    "        self.mlp_model.fit(X, y)\n",
    "\n",
    "    def predict(self, n: int) -> TimeSeries:\n",
    "        arima_forecast = TimeSeries.from_series(\n",
    "            self.arima_model.predict(n_periods=n))\n",
    "        input_residuals = self.last_residuals.values.reshape(1, -1)\n",
    "        residual_forecast_values = self.mlp_model.predict(\n",
    "            input_residuals).flatten()\n",
    "        residual_forecast = TimeSeries.from_times_and_values(\n",
    "            times=arima_forecast.time_index, values=residual_forecast_values, columns=arima_forecast.columns)\n",
    "        return arima_forecast + residual_forecast\n",
    "\n",
    "\n",
    "class ARIMA_LSTM(BaseModel):\n",
    "    def __init__(self, name=\"ARIMA-LSTM\", n_lags=24, n_epochs=100):\n",
    "        super().__init__(name)\n",
    "        self.n_lags, self.n_epochs = n_lags, n_epochs\n",
    "        self.arima_model, self.lstm_model, self.residuals_train = None, None, None\n",
    "        self.scaler = Scaler(MinMaxScaler(feature_range=(0, 1)))\n",
    "\n",
    "    def fit(self, train_series: TimeSeries, forecast_horizon: int):\n",
    "        self.arima_model = auto_arima(get_safe_pandas_series(\n",
    "            train_series), seasonal=False, stepwise=True, suppress_warnings=True)\n",
    "        residuals = self.arima_model.resid()\n",
    "        self.residuals_train = TimeSeries.from_series(residuals)\n",
    "        residuals_scaled = self.scaler.fit_transform(self.residuals_train)\n",
    "        self.lstm_model = RNNModel(model='LSTM', input_chunk_length=self.n_lags,\n",
    "                                   output_chunk_length=1, n_epochs=self.n_epochs, random_state=42)\n",
    "        self.lstm_model.fit(residuals_scaled)\n",
    "\n",
    "    def predict(self, n: int) -> TimeSeries:\n",
    "        arima_forecast = TimeSeries.from_series(\n",
    "            self.arima_model.predict(n_periods=n))\n",
    "        residual_forecast_scaled = self.lstm_model.predict(\n",
    "            n=n, series=self.scaler.transform(self.residuals_train))\n",
    "        residual_forecast = self.scaler.inverse_transform(\n",
    "            residual_forecast_scaled)\n",
    "        return arima_forecast + residual_forecast\n",
    "\n",
    "\n",
    "class PureLSTM(BaseModel):\n",
    "\n",
    "    def __init__(self, name=\"PureLSTM\", n_lags=24, n_epochs=100):\n",
    "        super().__init__(name)\n",
    "        self.n_lags, self.n_epochs = n_lags, n_epochs\n",
    "        self.scaler = Scaler(MinMaxScaler(feature_range=(0, 1)))\n",
    "        self.model = None\n",
    "\n",
    "    def fit(self, train_series: TimeSeries, forecast_horizon: int):\n",
    "        train_scaled = self.scaler.fit_transform(train_series)\n",
    "\n",
    "        self.model = RNNModel(\n",
    "            model='LSTM',\n",
    "            input_chunk_length=self.n_lags,\n",
    "            output_chunk_length=1,  # Corrigido para 1\n",
    "            n_epochs=self.n_epochs,\n",
    "            random_state=42\n",
    "        )\n",
    "        self.model.fit(train_scaled)\n",
    "\n",
    "    def predict(self, n: int) -> TimeSeries:\n",
    "        # 1. Prever na escala normalizada\n",
    "        prediction_scaled = self.model.predict(n=n)\n",
    "        \n",
    "        # 2. Reverter para a escala original antes de retornar\n",
    "        return self.scaler.inverse_transform(prediction_scaled)\n",
    "\n",
    "\n",
    "class PureNBEATS(BaseModel):\n",
    "    def __init__(self, name=\"PureNBEATS\", n_lags=24, n_epochs=100):\n",
    "        super().__init__(name)\n",
    "        self.n_lags, self.n_epochs = n_lags, n_epochs\n",
    "        self.scaler = Scaler(MinMaxScaler(feature_range=(0, 1)))\n",
    "        self.model = None\n",
    "\n",
    "    def fit(self, train_series: TimeSeries, forecast_horizon: int):\n",
    "        train_scaled = self.scaler.fit_transform(train_series)\n",
    "        self.model = NBEATSModel(input_chunk_length=self.n_lags,\n",
    "                                 output_chunk_length=forecast_horizon, n_epochs=self.n_epochs, random_state=42)\n",
    "        self.model.fit(train_scaled)\n",
    "\n",
    "    def predict(self, n: int) -> TimeSeries:\n",
    "        # 1. Prever na escala normalizada\n",
    "        prediction_scaled = self.model.predict(n=n)\n",
    "        \n",
    "        # 2. Reverter para a escala original antes de retornar\n",
    "        return self.scaler.inverse_transform(prediction_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "07f2f9d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# PASSO 4: CLASSE EXPERIMENT RUNNER\n",
    "# ===================================================================\n",
    "class ExperimentRunner:\n",
    "    def __init__(self, datasets: dict, models: list, forecast_horizon: int):\n",
    "        self.datasets, self.models, self.forecast_horizon = datasets, models, forecast_horizon\n",
    "\n",
    "    def run(self):\n",
    "        os.makedirs('results/predictions', exist_ok=True)\n",
    "        os.makedirs('results/metrics', exist_ok=True)\n",
    "        for ds_name, series in tqdm(self.datasets.items(), desc=\"Processando Datasets\"):\n",
    "            train, test = series[:-\n",
    "                                 self.forecast_horizon], series[-self.forecast_horizon:]\n",
    "            for model in tqdm(self.models, desc=f\"Modelos para {ds_name}\", leave=False):\n",
    "                try:\n",
    "                    model.fit(train, self.forecast_horizon)\n",
    "                    prediction = model.predict(self.forecast_horizon)\n",
    "                    pred_df = prediction.to_series().to_frame(name='prediction')\n",
    "                    pred_df.to_csv(\n",
    "                        f'results/predictions/{ds_name}_{model.name}.csv')\n",
    "                    mape_score = mape(test, prediction)\n",
    "                    mase_score = mase(test, prediction, train)\n",
    "                    metrics = {'MAPE': mape_score, 'MASE': mase_score}\n",
    "                    with open(f'results/metrics/{ds_name}_{model.name}.pkl', 'wb') as f:\n",
    "                        pickle.dump(metrics, f)\n",
    "                    print(\n",
    "                        f\"Resultados para {model.name} em {ds_name}: MAPE={mape_score:.2f}%, MASE={mase_score:.3f}\")\n",
    "                except Exception as e:\n",
    "                    print(\n",
    "                        f\"ERRO ao processar o modelo {model.name} no dataset {ds_name}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f99b1a7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Carregando o dataset 'Sunspots' da biblioteca e salvando localmente...\n",
      "\n",
      "\n",
      "--- INICIANDO A EXECUÇÃO DOS EXPERIMENTOS ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74be01819727478aa2e73c389629f438",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processando Datasets:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01926616c1964a45a121736dd4ad61bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Modelos para Sunspots:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name            | Type             | Params | Mode \n",
      "-------------------------------------------------------------\n",
      "0 | criterion       | MSELoss          | 0      | train\n",
      "1 | train_criterion | MSELoss          | 0      | train\n",
      "2 | val_criterion   | MSELoss          | 0      | train\n",
      "3 | train_metrics   | MetricCollection | 0      | train\n",
      "4 | val_metrics     | MetricCollection | 0      | train\n",
      "5 | stacks          | ModuleList       | 6.2 M  | train\n",
      "-------------------------------------------------------------\n",
      "6.2 M     Trainable params\n",
      "1.4 K     Non-trainable params\n",
      "6.2 M     Total params\n",
      "24.780    Total estimated model params size (MB)\n",
      "396       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed1d87bb029c4b69be6ae73ea103ad24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name            | Type             | Params | Mode \n",
      "-------------------------------------------------------------\n",
      "0 | criterion       | MSELoss          | 0      | train\n",
      "1 | train_criterion | MSELoss          | 0      | train\n",
      "2 | val_criterion   | MSELoss          | 0      | train\n",
      "3 | train_metrics   | MetricCollection | 0      | train\n",
      "4 | val_metrics     | MetricCollection | 0      | train\n",
      "5 | stacks          | ModuleList       | 6.2 M  | train\n",
      "-------------------------------------------------------------\n",
      "6.2 M     Trainable params\n",
      "1.4 K     Non-trainable params\n",
      "6.2 M     Total params\n",
      "24.780    Total estimated model params size (MB)\n",
      "396       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb5a9e0b3f1745bea83e98651a8db165",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name            | Type             | Params | Mode \n",
      "-------------------------------------------------------------\n",
      "0 | criterion       | MSELoss          | 0      | train\n",
      "1 | train_criterion | MSELoss          | 0      | train\n",
      "2 | val_criterion   | MSELoss          | 0      | train\n",
      "3 | train_metrics   | MetricCollection | 0      | train\n",
      "4 | val_metrics     | MetricCollection | 0      | train\n",
      "5 | stacks          | ModuleList       | 6.2 M  | train\n",
      "-------------------------------------------------------------\n",
      "6.2 M     Trainable params\n",
      "1.4 K     Non-trainable params\n",
      "6.2 M     Total params\n",
      "24.781    Total estimated model params size (MB)\n",
      "396       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f3ef5af5330403ba963cd47e1220cc6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name            | Type             | Params | Mode \n",
      "-------------------------------------------------------------\n",
      "0 | criterion       | MSELoss          | 0      | train\n",
      "1 | train_criterion | MSELoss          | 0      | train\n",
      "2 | val_criterion   | MSELoss          | 0      | train\n",
      "3 | train_metrics   | MetricCollection | 0      | train\n",
      "4 | val_metrics     | MetricCollection | 0      | train\n",
      "5 | stacks          | ModuleList       | 6.2 M  | train\n",
      "-------------------------------------------------------------\n",
      "6.2 M     Trainable params\n",
      "1.4 K     Non-trainable params\n",
      "6.2 M     Total params\n",
      "24.782    Total estimated model params size (MB)\n",
      "396       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e11dd72d9ef544dba46f565b964ddf2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name            | Type             | Params | Mode \n",
      "-------------------------------------------------------------\n",
      "0 | criterion       | MSELoss          | 0      | train\n",
      "1 | train_criterion | MSELoss          | 0      | train\n",
      "2 | val_criterion   | MSELoss          | 0      | train\n",
      "3 | train_metrics   | MetricCollection | 0      | train\n",
      "4 | val_metrics     | MetricCollection | 0      | train\n",
      "5 | stacks          | ModuleList       | 6.2 M  | train\n",
      "-------------------------------------------------------------\n",
      "6.2 M     Trainable params\n",
      "1.4 K     Non-trainable params\n",
      "6.2 M     Total params\n",
      "24.782    Total estimated model params size (MB)\n",
      "396       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54fb3c874d96474cb7e3e0ec6d104c55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name            | Type             | Params | Mode \n",
      "-------------------------------------------------------------\n",
      "0 | criterion       | MSELoss          | 0      | train\n",
      "1 | train_criterion | MSELoss          | 0      | train\n",
      "2 | val_criterion   | MSELoss          | 0      | train\n",
      "3 | train_metrics   | MetricCollection | 0      | train\n",
      "4 | val_metrics     | MetricCollection | 0      | train\n",
      "5 | stacks          | ModuleList       | 6.2 M  | train\n",
      "-------------------------------------------------------------\n",
      "6.2 M     Trainable params\n",
      "1.4 K     Non-trainable params\n",
      "6.2 M     Total params\n",
      "24.783    Total estimated model params size (MB)\n",
      "396       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "141f33dd12ea442d8b5d111d7a91e90b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name            | Type             | Params | Mode \n",
      "-------------------------------------------------------------\n",
      "0 | criterion       | MSELoss          | 0      | train\n",
      "1 | train_criterion | MSELoss          | 0      | train\n",
      "2 | val_criterion   | MSELoss          | 0      | train\n",
      "3 | train_metrics   | MetricCollection | 0      | train\n",
      "4 | val_metrics     | MetricCollection | 0      | train\n",
      "5 | stacks          | ModuleList       | 6.2 M  | train\n",
      "-------------------------------------------------------------\n",
      "6.2 M     Trainable params\n",
      "1.4 K     Non-trainable params\n",
      "6.2 M     Total params\n",
      "24.784    Total estimated model params size (MB)\n",
      "396       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "404430c44937469ab0bbd19b2c081d7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name            | Type             | Params | Mode \n",
      "-------------------------------------------------------------\n",
      "0 | criterion       | MSELoss          | 0      | train\n",
      "1 | train_criterion | MSELoss          | 0      | train\n",
      "2 | val_criterion   | MSELoss          | 0      | train\n",
      "3 | train_metrics   | MetricCollection | 0      | train\n",
      "4 | val_metrics     | MetricCollection | 0      | train\n",
      "5 | stacks          | ModuleList       | 6.2 M  | train\n",
      "-------------------------------------------------------------\n",
      "6.2 M     Trainable params\n",
      "1.4 K     Non-trainable params\n",
      "6.2 M     Total params\n",
      "24.785    Total estimated model params size (MB)\n",
      "396       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "547a12a06acf41eb938a30c312811885",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name            | Type             | Params | Mode \n",
      "-------------------------------------------------------------\n",
      "0 | criterion       | MSELoss          | 0      | train\n",
      "1 | train_criterion | MSELoss          | 0      | train\n",
      "2 | val_criterion   | MSELoss          | 0      | train\n",
      "3 | train_metrics   | MetricCollection | 0      | train\n",
      "4 | val_metrics     | MetricCollection | 0      | train\n",
      "5 | stacks          | ModuleList       | 6.2 M  | train\n",
      "-------------------------------------------------------------\n",
      "6.2 M     Trainable params\n",
      "1.4 K     Non-trainable params\n",
      "6.2 M     Total params\n",
      "24.785    Total estimated model params size (MB)\n",
      "396       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f771fc44260409995e66b7104c3146c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name            | Type             | Params | Mode \n",
      "-------------------------------------------------------------\n",
      "0 | criterion       | MSELoss          | 0      | train\n",
      "1 | train_criterion | MSELoss          | 0      | train\n",
      "2 | val_criterion   | MSELoss          | 0      | train\n",
      "3 | train_metrics   | MetricCollection | 0      | train\n",
      "4 | val_metrics     | MetricCollection | 0      | train\n",
      "5 | stacks          | ModuleList       | 6.2 M  | train\n",
      "-------------------------------------------------------------\n",
      "6.2 M     Trainable params\n",
      "1.4 K     Non-trainable params\n",
      "6.2 M     Total params\n",
      "24.786    Total estimated model params size (MB)\n",
      "396       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3833f04b799b4c46ac1eb7f96676be6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60da9210e3bf4f39b1ca08596948296e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b6ac3c0a20b40a487f428f66f7af7c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be0b5037bf6a4ae7bca12ba2e17e592a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13303538dbeb48249c7a3009128d164a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfbc19e5c5354368afeb43ee057d288e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "611a8cf2a8ed4fb69003b9590e0880bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e6c355741a748f0bc62e03f6ce9ad49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ed6f0035be340dabc40b09b5faed7b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50e96d84061b4879b1fe792a6b1b2523",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33c0e6b7c6124446ba0b07adb4f68e94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultados para HyS-MF em Sunspots: MAPE=145.92%, MASE=0.590\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name            | Type             | Params | Mode \n",
      "-------------------------------------------------------------\n",
      "0 | criterion       | MSELoss          | 0      | train\n",
      "1 | train_criterion | MSELoss          | 0      | train\n",
      "2 | val_criterion   | MSELoss          | 0      | train\n",
      "3 | train_metrics   | MetricCollection | 0      | train\n",
      "4 | val_metrics     | MetricCollection | 0      | train\n",
      "5 | stacks          | ModuleList       | 6.2 M  | train\n",
      "-------------------------------------------------------------\n",
      "6.2 M     Trainable params\n",
      "1.4 K     Non-trainable params\n",
      "6.2 M     Total params\n",
      "24.786    Total estimated model params size (MB)\n",
      "396       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4ea6c27a62847cdb873df5c464a19e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e5f6899706f45f1aea9dc6872aa87fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultados para HyS-MF_MIMO em Sunspots: MAPE=147.24%, MASE=0.718\n",
      "Resultados para ARIMA-MLP em Sunspots: MAPE=186.72%, MASE=0.880\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ignoring user defined `output_chunk_length`. RNNModel uses a fixed `output_chunk_length=1`.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name            | Type             | Params | Mode \n",
      "-------------------------------------------------------------\n",
      "0 | criterion       | MSELoss          | 0      | train\n",
      "1 | train_criterion | MSELoss          | 0      | train\n",
      "2 | val_criterion   | MSELoss          | 0      | train\n",
      "3 | train_metrics   | MetricCollection | 0      | train\n",
      "4 | val_metrics     | MetricCollection | 0      | train\n",
      "5 | rnn             | LSTM             | 2.8 K  | train\n",
      "6 | V               | Linear           | 26     | train\n",
      "-------------------------------------------------------------\n",
      "2.8 K     Trainable params\n",
      "0         Non-trainable params\n",
      "2.8 K     Total params\n",
      "0.011     Total estimated model params size (MB)\n",
      "7         Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1af806c1b774d2caba43ee6979d28dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf72cf2881694937ba1eb286e04cef09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultados para ARIMA-LSTM em Sunspots: MAPE=192.99%, MASE=0.689\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ignoring user defined `output_chunk_length`. RNNModel uses a fixed `output_chunk_length=1`.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name            | Type             | Params | Mode \n",
      "-------------------------------------------------------------\n",
      "0 | criterion       | MSELoss          | 0      | train\n",
      "1 | train_criterion | MSELoss          | 0      | train\n",
      "2 | val_criterion   | MSELoss          | 0      | train\n",
      "3 | train_metrics   | MetricCollection | 0      | train\n",
      "4 | val_metrics     | MetricCollection | 0      | train\n",
      "5 | rnn             | LSTM             | 2.8 K  | train\n",
      "6 | V               | Linear           | 26     | train\n",
      "-------------------------------------------------------------\n",
      "2.8 K     Trainable params\n",
      "0         Non-trainable params\n",
      "2.8 K     Total params\n",
      "0.011     Total estimated model params size (MB)\n",
      "7         Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultados para PureARIMA em Sunspots: MAPE=172.77%, MASE=0.651\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "533345aa187046e1b6e705a2f6557a09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4caa94e67f574795817071ce50d872d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultados para PureLSTM em Sunspots: MAPE=155.89%, MASE=0.585\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name            | Type             | Params | Mode \n",
      "-------------------------------------------------------------\n",
      "0 | criterion       | MSELoss          | 0      | train\n",
      "1 | train_criterion | MSELoss          | 0      | train\n",
      "2 | val_criterion   | MSELoss          | 0      | train\n",
      "3 | train_metrics   | MetricCollection | 0      | train\n",
      "4 | val_metrics     | MetricCollection | 0      | train\n",
      "5 | stacks          | ModuleList       | 6.2 M  | train\n",
      "-------------------------------------------------------------\n",
      "6.2 M     Trainable params\n",
      "1.4 K     Non-trainable params\n",
      "6.2 M     Total params\n",
      "24.786    Total estimated model params size (MB)\n",
      "396       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6b69cbe25ea4fecada639e5d5f37725",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c52ca7618ce486da456b8ac6ff4a799",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultados para PureNBEATS em Sunspots: MAPE=101.57%, MASE=0.684\n",
      "\n",
      "\n",
      "--- TODOS OS EXPERIMENTOS FORAM CONCLUÍDOS ---\n"
     ]
    }
   ],
   "source": [
    "# ===================================================================\n",
    "# PASSO 5: EXECUÇÃO DO EXPERIMENTO\n",
    "# ===================================================================\n",
    "# LISTA_DE_DATASETS = ['AirPassengers', 'co2', 'UKgas', 'Sunspots','ukdriverdeaths']\n",
    "LISTA_DE_DATASETS = ['Sunspots']\n",
    "                     \n",
    "data_loader = DataLoader()\n",
    "datasets_darts = {}\n",
    "filler = MissingValuesFiller()\n",
    "for name in LISTA_DE_DATASETS:\n",
    "    pd_series = data_loader.load_classic_ts_dataset(name)\n",
    "    if pd_series is not None:\n",
    "        series_darts = TimeSeries.from_series(\n",
    "            pd_series, fill_missing_dates=True, freq=None)\n",
    "        series_darts = filler.transform(series_darts)\n",
    "        datasets_darts[name] = series_darts\n",
    "\n",
    "models_to_run = [\n",
    "    # --- MODELO PRINCIPAL E VARIAÇÕES ---\n",
    "    HySMF(),               # Modelo Original do Artigo (ARIMA: Recursivo / N-BEATS: Direto)\n",
    "    HyS_MF_MIMO(name=\"HyS-MF_MIMO\"), # Variação 1 (ARIMA: Recursivo / N-BEATS: MIMO)\n",
    "\n",
    "    # --- HÍBRIDOS DE COMPARAÇÃO DA LITERATURA ---\n",
    "    ARIMA_MLP(),           # Híbrido (ARIMA: Recursivo / MLP: MIMO)\n",
    "    ARIMA_LSTM(),          # Híbrido (ARIMA: Recursivo / LSTM: Recursivo, devido à biblioteca)\n",
    "\n",
    "    # # --- BASELINES PUROS ---\n",
    "    PureARIMA(),           # Baseline Linear (Estratégia Recursiva)\n",
    "    PureLSTM(),            # Baseline Não-Linear (Estratégia Recursiva, devido à biblioteca)\n",
    "    PureNBEATS()           # Baseline Não-Linear (Estratégia MIMO)\n",
    "]\n",
    "\n",
    "FORECAST_HORIZON = 10\n",
    "runner = ExperimentRunner(datasets_darts, models_to_run, FORECAST_HORIZON)\n",
    "\n",
    "print(\"\\n\\n--- INICIANDO A EXECUÇÃO DOS EXPERIMENTOS ---\")\n",
    "runner.run()\n",
    "print(\"\\n\\n--- TODOS OS EXPERIMENTOS FORAM CONCLUÍDOS ---\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vhybrid_clean",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
